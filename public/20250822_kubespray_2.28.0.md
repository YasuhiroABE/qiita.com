---
title: Kubespray v2.28.1でクラスターをアップグレードした時のメモ
tags:
  - Ansible
  - kubernetes
  - kubespray
private: false
updated_at: '2025-12-09T12:05:19+09:00'
id: 6a16e3e9ac4f027bd559
organization_url_name: null
slide: false
ignorePublish: false
---
# はじめに

これまで1つの記事に各バージョンの情報を含めてきましたが、さすがに読みにくいため個別の記事に分割することにしました。

https://qiita.com/YasuhiroABE/items/3aaf7ceb314f47cd62d9

Kubespray v2.28.0を適用します。

【2025/8/28追記】
v2.28.1がリリースされたので、これを同様の手順で適用しています。

## 環境

* サーバー: TX120 S3p (CPU: Xeon e3-1230 v2, Memory: 32GB)

### 適用対象の現行バージョン

* Kubespray v2.27.0 (Kubernetes 1.31.4)
* Kubespray v2.27.1 (Kubernetes 1.31.9)

# 参考資料

https://github.com/kubernetes-sigs/kubespray/blob/303dd1cbc16620ff666f111e59add9d3965a449d/docs/operations/upgrades.md

# 事前確認

作業前に次の点を確認します。

## クラスターの再起動

直近で再起動をしていない場合には最新のカーネルに更新(``sudo apt update && sudo apt upgrade``)した上で再起動します。

```bash:control-planeノードで事前にdorainする
$ sudo kubectl cordon node4
$ sudo kubectl drain node4 --force --ignore-daemonsets
## ↑で失敗した場合はメッセージを確認して"--delete-emptydir-data"オプション付きで実行する
$ sudo kubectl drain node4 --force --ignore-daemonsets --delete-emptydir-data
```

対象ノードを再起動します。

```bash:対象ノード(node4)にssh等でログインし、再起動
$ sudo shutdown -r now
```

再起動してからクラスターに復帰させます。

```bash:control-planeノードでuncordonする
$ sudo kubectl uncordon node4
```

:::note
経験上、再起動せずにansible-playbookを進めるとkubernetes/preinstallのプロセスでハングアップすることがありました。
:::

## RELEASE_NOTESの確認

必ず[Release Notes](https://github.com/kubernetes-sigs/kubespray/releases)の各バージョンに記載されている変更内容を確認してください。

v2.28.0では全てのコンポーネントのバージョン番号の指定方法が"v"を先頭に付けない文字列に変更されました。

## Rook/Cephの状態確認

対象のクラスターではPVCを利用するためにRook/Cephを利用しています。

Rook/Cephを利用している場合には、あらかじめクラスターの状態が、**HEALTH_OK** であることを確認します。

```bash:
$ podname=$(kubectl -n rook-ceph get pod -l app=rook-ceph-tools -o jsonpath='{.items[*].metadata.name}')
$ kubectl -n rook-ceph exec -it "${podname}" -- ceph status
```

## 各ノードが参照するAPI Serverのホスト名を確認

過去にaddons.ymlファイルでkube_vip_services_enabled:を有効にするなどしていると作業中に障害となる可能性があります。

各ノードのAPI ServerへのURLを確認します。

lb-apiserver.kubernetes.local を指している場合には、作業前に**localhost**や**127.0.0.1**に変更します。

```bash:各ノードで/etc/kubernetes/以下のファイルからAPIサーバーのURLを確認
## kubesprayのansible-playbookを実行しているディレクトリで実行
$ . venv/ansible/bin/activate
(k8s) $ ansible all -i inventory/mycluster/inventory.ini -b -m shell -a 'grep server: /etc/kubernetes/*.conf'
```

:::note
kubeletがAPI serverと通信できなくなった場合は/etc/kubernetes/*.confファイルの中からVIP(lb-apiserver.kubernetes.local)を参照している箇所を修正して、systemctlからkubeletを再起動します。

問題が起ってからの対応でも間に合いますが、そのままansible-playbookを実行するのは危険なので、

具体的な対応方法についてはこの記事の後半に追記しています。
:::


# 作業手順

基本的な遵守事項は次のとおり。

* masterブランチに変更は加えないこと
* masterブランチから作業用ブランチは作成しないこと
* 作業の度にtagsから作業用ブランチを作成すること
* 変更内容は作業用ブランチにcommitすること

始めて実施する場合にはあらかじめGithubからkubesprayディレクトリをpullすること。

```bash:初回のみ
$ git pull https://github.com/kubernetes-sigs/kubespray.git
```

通常はkubesprayディレクトリが存在するものとして次のステップから開始する。

```bash:
$ cd kubespray
$ git status
## コミットしていないファイルがあるとブランチの作成ができないため、untrackedやchangedなファイルがあればcommitしておく
## masterブランチにcommitしないため日付のブランチを作成してcommitする
$ git checkout -b $(date +%Y%m%d.%H%M%S)_tmp
$ git commit -a -m 'prepare to upgrade v2.28.1'

## 【git statusで差分が表示されなければ、ここからスタート】まずkubesprayの最新のリポジトリに更新
$ git checkout master
$ git pull
## 次のバージョンタグを確認する
$ git tag
## v2.27.xの次のv2.28.1からブランチを作成する
$ git checkout refs/tags/v2.28.1 -b t_v2.28.1

## 新しいvenv環境を作成し、環境変数を読み込む
$ rm -rf venv
$ /usr/bin/python3 -m venv venv/k8s
$ . venv/k8s/bin/activate
(k8s) $ pip install -r requirements.txt

(k8s) ## inventory/mycluster を作成する。(もし inventory/mycluster があれば削除する)
(k8s) $ rm -rf inventory/mycluster
(k8s) $ cp -rfp inventory/sample inventory/mycluster
## v2.27.0以降ではinventory/mycluster/inventory.iniファイルを利用する
(k8s) $ git checkout t_v2.27.0 inventory/mycluster/inventory.ini
## .gitignore に !inventory/mycluster を追加する
(k8s) $ vi .gitignore
(k8s) $ git add .gitignore inventory/mycluster
(k8s) $ git commit -m 'Added the inventory/mycluster config directory.'
## 一つ前のバージョン(v2.27.0)との差分を確認する
(k8s) $ git diff t_v2.27.0 inventory/mycluster
## 変更が必要なファイルは group_vars/k8s-cluster/{addons.yml,k8s-cluster.yml,k8s-net-calico.yml} のみです
## container_manager: が変更になるなどの大規模な変更がないか、念のため全体を概観してください
## 変更する際には各ファイルの差分を確認しながら作業を進めていきます
(k8s) $ cd inventory/mycluster/group_vars/k8s_cluster
(k8s) $ git diff t_v2.27.0 addons.yml
(k8s) $ vi addons.yml
(k8s) $ git diff t_v2.27.0 k8s-cluster.yml
(k8s) $ vi k8s-cluster.yml
(k8s) $ git diff t_v2.27.0 k8s-net-calico.yml
(k8s) $ vi k8s-net-calico.yml
## 変更が終ったら差分をコミットし、トップディレクトリに移動します
(k8s) $ git add .
(k8s) $ git commit -m 'Modified addons.yml, k8s-cluster.yml, and k8s-net-calico.yml files.'
(k8s) $ cd ../../../../
## 変更方法をupgrades.mdから確認する。そのままでは古いコマンドなので適宜変更すること
(k8s) $ less docs/operations/upgrades.md
## ansible.cfgを見直し、remote_userなどを環境に合わせて、適宜変更する
(k8s) $ git diff t_v2.27.0 ansible.cfg
## 変更したinventory.iniが前の版から変化していないことを確認する (v2.28.0以降は一つ前と比較する)
(k8s) $ git diff t_v2.27.0 inventory/mycluster/inventory.ini
## 変更したansible.cfgが正常に動くか動作を確認し、同時に"kube_node"などに対応するノードの名前と数が正しいか確認する
(k8s) $ ansible kube_control_plane -i inventory/mycluster/inventory.ini -m command -a 'uname -n'
(k8s) $ ansible etcd -i inventory/mycluster/inventory.ini -m command -a 'uname -n'
(k8s) $ ansible kube_node -i inventory/mycluster/inventory.ini -m command -a 'uname -n'
## 正常に動作したら残りのファイルをコミットする (変更点があればinventory.iniも)
(k8s) $ git add ansible.cfg inventory/mycluster/inventory.ini
(k8s) $ git commit -m 'Updated the configuration for Ansible.'
## kube_version を確認するが、v2.28.0から設定ファイルにデフォルト値が記載されなくなったので、RELEASE_NOTESか、checksums.ymlファイルからサポートされているバージョンを確認する
(k8s) $ grep -C 40 kubelet_checksums roles/kubespray_defaults/vars/main/checksums.yml
## 確認したkube_versionを指定して、upgrade-cluster.ymlを実行する
(k8s) $ ansible-playbook upgrade-cluster.yml -b -i inventory/mycluster/inventory.ini -e kube_version=1.32.8
```

# 既知の問題への対応方法

```bash:upgrade-cluster.ymlの実行に失敗した状態
$ kubectl get node
NAME    STATUS                     ROLES           AGE      VERSION
node1   Ready                      control-plane   4y218d   v1.32.5
node2   Ready                      control-plane   4y218d   v1.32.5
node3   Ready                      <none>          4y218d   v1.31.4
node4   Ready,SchedulingDisabled   <none>          4y128d   v1.31.4
node5   Ready                      <none>          4y128d   v1.31.4
```

エラーになっている理由は、cordonした後に想定している時間内にSchedulingDisabledとならなかった点にありました。
積極的にcordon/uncordonを利用して1台づつ処理する場合は次のような流れになります。

```bash:手動で1台づつupgrade-cluster.ymlを実行する例
## kubectlコマンドが実行できるホストに移動
$ kubectl cordon node4

## ansibleを実行するホストに移動し、--limitを利用してcordonしたノードのみにansible roleを適用する
$ ansible-playbook upgrade-cluster.yml -b -i inventory/mycluster/inventory.ini -e kube_version=v1.39.5 --limit node4

## 再びkubectlコマンドを実行するホストに移動
$ kubectl uncordon node2
```

この他のアップグレードが完了していないノードについても、corodnしてから--limitでそのノードを指定してアップグレードし、uncordonしてクラスターに参加、という流れを繰り返します。

この時にRook/Cephを利用していれば、次のノードに移る前に``ceph status``でHEALTH_OKの状態を都度確認してから作業を続けます。

## どうしてもCordonできないノードが発生した時の対応

前述の方法でだいたいは対応ができますが、どうしてもcordon/drainの処理に失敗する場合があります。

```bash:手動で1台づつupgrade-cluster.ymlを実行する例
## kubectlコマンドが実行できるホストに移動
$ kubectl cordon node4

## drainできるか確認する
$ kubectl drain node4 --force --ignore-daemonsets --delete-emptydir-data

## 【注意:別ホストでの作業】 ansibleを実行するホストに移動し、--limitを利用してcordonしたノードのみにansible roleを適用する
$ ansible-playbook upgrade-cluster.yml -b -i inventory/mycluster/inventory.ini -e kube_version=v1.39.5 --limit node4

## 再びkubectlコマンドを実行するホストに移動
$ kubectl uncordon node4
```

今回はRook/Cephのpdbが問題の原因でした。

```bash:
$ kubectl -n rook-ceph get pdb
NAME                       MIN AVAILABLE   MAX UNAVAILABLE   ALLOWED DISRUPTIONS   AGE
rook-ceph-mds-myfs         1               N/A               1                     233d
rook-ceph-mgr-pdb          N/A             1                 1                     233d
rook-ceph-mon-pdb          N/A             1                 0                     233d
rook-ceph-osd-host-node1   N/A             0                 0                     58m
rook-ceph-osd-host-node3   N/A             0                 0                     58m
rook-ceph-osd-host-node4   N/A             0                 0                     58m
rook-ceph-osd-host-node5   N/A             0                 0                     58m
```

手動で該当のpdbリソースをeditコマンドで ``maxUnavailable: 1`` に変更して、とりあえずevictできるようにしています。

```bash:
$ kubectl -n rook-ceph edit pdb rook-ceph-osd-host-node4
```

変更後は次のようになっています。

```bash:
$ kubectl -n rook-ceph get pdb
NAME                       MIN AVAILABLE   MAX UNAVAILABLE   ALLOWED DISRUPTIONS   AGE
rook-ceph-mds-myfs         1               N/A               1                     233d
rook-ceph-mgr-pdb          N/A             1                 1                     233d
rook-ceph-mon-pdb          N/A             1                 0                     233d
rook-ceph-osd-host-node1   N/A             0                 0                     58m
rook-ceph-osd-host-node3   N/A             0                 0                     58m
rook-ceph-osd-host-node4   N/A             1                 1                     58m
rook-ceph-osd-host-node5   N/A             0                 0                     58m
```

この状態で冒頭の手順を繰り返し実行することで、node4を更新することができるようになりました。

他のノードでも同様の問題が発生するため、繰り返し対応していきます。


# 適用後に発生した障害

## API Serverが参照できないことによる障害

:::note
問題なく更新に成功しているKube-VIPを利用しているクラスターもあります。
:::

事前の確認項目にも含めていますが、作業中に先頭ノード以外が全てNotReady状態になる現象が発生しました。

```text:
NAME    STATUS                        ROLES           AGE      VERSION
node1   Ready                         control-plane   3y356d   v1.32.5
node2   NotReady,SchedulingDisabled   control-plane   3y356d   v1.31.4
node3   NotReady                      <none>          3y356d   v1.31.4
node4   NotReady                      <none>          3y356d   v1.31.4
```

これは/etc/kubernetes/kubelet.confファイルの.clusters[*].cluster.server情報が書き変わり、VIPが構成するアドレス ``https://lb-apiserver.kubernetes.local:6443`` に書き変わってしまっていたためです。

詳細は次の記事を参照してください。

https://qiita.com/YasuhiroABE/items/004f7c6413168dfd0792

NotReadyとなったノードの/etc/kuberenetes/ディレクトリの構成ファイル中からlb-apiserver.kubernetes.localのホスト名をlocalhostや127.0.0.1に書き換えてから、再度ansible-playbookを--limitオプションを付けて実行しています。

inventory/mycluster/group_vars/k8s_cluster/addons.ymlファイルでは、kube_vip_services_enabled:はfalseになっていましたが、過去に環境を構築する際にtrueに設定するなどして全てのAPIサーバーの参照先がlb-apiserver.kubernetes.localになっていたようです。

因果関係は検証できていませんが、使っていないと思っていたホスト名(lb-apiserver.kubernetes.local)に対応するVIPアドレスを変更途中までnodelocaldnsが正常に返していたのに、それを止めてしまったので障害になったのだと思われます。

時間があればVMwareのテスト環境で再現するか確認してみたいと思います。

## Cordonできないノードによる障害

サービスへの影響はありませんでしたが、検証用のテスト環境では遭遇しなかった現象がRook/Cephで発生しました。

対応手順は一般的な作業手順にまとめているので、ここでは原因についてだけまとめておきます。

namespace: rook-ceph のPDBリソースの設定に食い違いがありました。

```bash:テスト環境のPDBリソースの設定状況 (問題なく1.32.5にアップグレードできたクラスター)
$ kubectl -n rook-ceph get pdb
NAME                 MIN AVAILABLE   MAX UNAVAILABLE   ALLOWED DISRUPTIONS   AGE
rook-ceph-mds-myfs   1               N/A               1                     4y10d
rook-ceph-mgr-pdb    N/A             1                 1                     233d
rook-ceph-mon-pdb    N/A             1                 1                     4y10d
rook-ceph-osd        N/A             1                 1                     20h
```

問題が発生した本番系では次のようになっていました。

```bash:
$ kubectl -n rook-ceph get pdb
NAME                       MIN AVAILABLE   MAX UNAVAILABLE   ALLOWED DISRUPTIONS   AGE
rook-ceph-mds-myfs         1               N/A               1                     233d
rook-ceph-mgr-pdb          N/A             1                 1                     233d
rook-ceph-mon-pdb          N/A             1                 0                     233d
rook-ceph-osd-host-node1   N/A             0                 0                     67m
rook-ceph-osd-host-node3   N/A             0                 0                     67m
rook-ceph-osd-host-node4   N/A             0                 0                     67m
rook-ceph-osd-host-node5   N/A             0                 0                     67m
```

node1は問題なくアップグレードが完了しているのがおかしな点ではあるのですが、node[345]について問題が発生しました。

対応策については前述したとおり、editコマンドで ``maxUnavailable: 1`` を設定して問題を回避しています。


