---
title: Kubespray v2.28.0でクラスターをアップグレードした時のメモ
tags:
  - Ansible
  - kubernetes
  - kubespray
private: false
updated_at: '2025-08-23T22:54:19+09:00'
id: 6a16e3e9ac4f027bd559
organization_url_name: null
slide: false
ignorePublish: false
---
# はじめに

これまで1つの記事に各バージョンの情報を含めてきましたが、さすがに読みにくいため個別の記事に分割することにしました。

https://qiita.com/YasuhiroABE/items/3aaf7ceb314f47cd62d9


Kubespray v2.28.0を適用します。

## 環境

* サーバー: TX120 S3p (CPU: Xeon e3-1230 v2, Memory: 32GB)

### 適用対象の現行バージョン

Kubespray v2.27.0 (Kubernetes 1.31.4)

# 参考資料

https://github.com/kubernetes-sigs/kubespray/blob/303dd1cbc16620ff666f111e59add9d3965a449d/docs/operations/upgrades.md

# 事前確認

作業前に次の点を確認します。

## RELEASE_NOTESの確認

必ず[Release Notes](https://github.com/kubernetes-sigs/kubespray/releases)の各バージョンに記載されている変更内容を確認してください。

v2.28.0では全てのコンポーネントのバージョン番号の指定方法が"v"を先頭に付けない文字列に変更されました。

## Rook/Cephの状態確認

クラスターの状態が、**HEALTH_OK** であることを確認します。

```bash:
$ podname=$(kubectl -n rook-ceph get pod -l app=rook-ceph-tools -o jsonpath='{.items[*].metadata.name}')
$ kubectl -n rook-ceph exec -it "${podname}" -- ceph status
```

## 各ノードが参照するAPI Serverのホスト名を確認

過去にaddons.ymlファイルでkube_vip_services_enabled:を有効にするなどしていると作業中に障害となる可能性があります。

各ノードのAPI ServerへのURLを確認します。

lb-apiserver.kubernetes.local を指している場合には、**localhost**や**127.0.0.1**に変更します。

```bash:各ノードで/etc/kubernetes/以下のファイルからAPIサーバーのURLを確認
## kubesprayのansible-playbookを実行しているディレクトリで実行
$ . venv/ansible/bin/activate
(k8s) $ ansible all -i inventory/mycluster/inventory.ini -b -m shell -a 'grep server: /etc/kubernetes/*.conf'
```

# 作業手順

基本的な遵守事項は次のとおり。

* masterブランチに変更は加えないこと
* masterブランチから作業用ブランチは作成しないこと
* 作業の度にtagsから作業用ブランチを作成すること
* 変更内容は作業用ブランチにcommitすること

始めて実施する場合にはあらかじめGithubからkubesprayディレクトリをpullすること。

```bash:初回のみ
$ git pull https://github.com/kubernetes-sigs/kubespray.git
```

通常はkubesprayディレクトリが存在するものとして次のステップから開始する。

```bash:
$ cd kubespray
$ git status
## コミットしていないファイルがあるとブランチの作成ができないため、untrackedやchangedなファイルがあればcommitしておく
## masterブランチにcommitしないため日付のブランチを作成してcommitする
$ git checkout -b $(date +%Y%m%d.%H%M%S)_tmp
$ git commit -a -m 'prepare to upgrade v2.28.0'

## 【git statusで差分が表示されなければ、ここからスタート】まずkubesprayの最新のリポジトリに更新
$ git checkout master
$ git pull
## 次のバージョンタグを確認する
$ git tag
## v2.27.xの次のv2.28.0からブランチを作成する
$ git checkout refs/tags/v2.28.0 -b t_v2.28.0

## 新しいvenv環境を作成し、環境変数を読み込む
$ rm -rf venv
$ /usr/bin/python3 -m venv venv/k8s
$ . venv/k8s/bin/activate
(k8s) $ pip install -r requirements.txt

(k8s) ## inventory/mycluster を作成する。(もし inventory/mycluster があれば削除する)
(k8s) $ rm -rf inventory/mycluster
(k8s) $ cp -rfp inventory/sample inventory/mycluster
## v2.27.0以降ではinventory/mycluster/inventory.iniファイルを利用する
(k8s) $ git checkout t_v2.27.0 inventory/mycluster/inventory.ini
## .gitignore に !inventory/mycluster を追加する
(k8s) $ vi .gitignore
(k8s) $ git add .gitignore inventory/mycluster
(k8s) $ git commit -m 'Added the inventory/mycluster config directory.'
## 一つ前のバージョン(v2.27.0)との差分を確認する
(k8s) $ git diff t_v2.27.0 inventory/mycluster
## 変更が必要なファイルは group_vars/k8s-cluster/{addons.yml,k8s-cluster.yml,k8s-net-calico.yml} のみです
## container_manager: が変更になるなどの大規模な変更がないか、念のため全体を概観してください
## 変更する際には各ファイルの差分を確認しながら作業を進めていきます
(k8s) $ cd inventory/mycluster/group_vars/k8s_cluster
(k8s) $ git diff t_v2.27.0 addons.yml
(k8s) $ vi addons.yml
(k8s) $ git diff t_v2.27.0 k8s-cluster.yml
(k8s) $ vi k8s-cluster.yml
(k8s) $ git diff t_v2.27.0 k8s-net-calico.yml
(k8s) $ vi k8s-net-calico.yml
## 変更が終ったら差分をコミットし、トップディレクトリに移動します
(k8s) $ git add .
(k8s) $ git commit -m 'Modified addons.yml, k8s-cluster.yml, and k8s-net-calico.yml files.'
(k8s) $ cd ../../../../
## 変更方法をupgrades.mdから確認する。そのままでは古いコマンドなので適宜変更すること
(k8s) $ less docs/operations/upgrades.md
## ansible.cfgを見直し、remote_userなどを環境に合わせて、適宜変更する
(k8s) $ git diff t_v2.27.0 ansible.cfg
## 変更したinventory.iniが前の版から変化していないことを確認する (v2.28.0以降は一つ前と比較する)
(k8s) $ git diff t_v2.27.0 inventory/mycluster/inventory.ini
## 変更したansible.cfgが正常に動くか動作を確認し、同時に"kube_node"などに対応するノードの名前と数が正しいか確認する
(k8s) $ ansible kube_control_plane -i inventory/mycluster/inventory.ini -m command -a 'uname -n'
(k8s) $ ansible etcd -i inventory/mycluster/inventory.ini -m command -a 'uname -n'
(k8s) $ ansible kube_node -i inventory/mycluster/inventory.ini -m command -a 'uname -n'
## 正常に動作したら残りのファイルをコミットする (変更点があればinventory.iniも)
(k8s) $ git add ansible.cfg inventory/mycluster/inventory.ini
(k8s) $ git commit -m 'Updated the configuration for Ansible.'
## kube_version を確認するが、v2.28.0から設定ファイルにデフォルト値が記載されなくなったので、RELEASE_NOTESか、checksums.ymlファイルからサポートされているバージョンを確認する
(k8s) $ grep -C 40 kubelet_checksums roles/kubespray_defaults/vars/main/checksums.yml
## 確認したkube_versionを指定して、upgrade-cluster.ymlを実行する
(k8s) $ ansible-playbook upgrade-cluster.yml -b -i inventory/mycluster/inventory.ini -e kube_version=1.32.5
```

# 適用後に発生した障害

## API Serverが参照できないことによる障害

事前の確認項目にも含めていますが、作業中に先頭ノード以外が全てNotReady状態になる現象が発生しました。

```text:
NAME       STATUS                        ROLES           AGE      VERSION
node1   Ready                         control-plane   3y356d   v1.32.5
node2   NotReady,SchedulingDisabled   control-plane   3y356d   v1.31.4
node3   NotReady                      <none>          3y356d   v1.31.4
node4   NotReady                      <none>          3y356d   v1.31.4
```

これは/etc/kubernetes/kubelet.confファイルの.clusters[*].cluster.server情報が書き変わり、VIPが構成するアドレス ``https://lb-apiserver.kubernetes.local:6443`` に書き変わってしまっていたためです。

詳細は次の記事を参照してください。

https://qiita.com/YasuhiroABE/items/004f7c6413168dfd0792

NotReadyとなったノードの/etc/kuberenetes/ディレクトリの構成ファイル中からlb-apiserver.kubernetes.localのホスト名をlocalhostや127.0.0.1に書き換えてから、再度ansible-playbookを--limitオプションを付けて実行しています。

inventory/mycluster/group_vars/k8s_cluster/addons.ymlファイルでは、kube_vip_services_enabled:はfalseになっていましたが、過去に環境を構築する際にtrueに設定するなどして全てのAPIサーバーの参照先がlb-apiserver.kubernetes.localになっていたようです。

因果関係は検証できていませんが、使っていないと思っていたホスト名(lb-apiserver.kubernetes.local)に対応するVIPアドレスを変更途中までnodelocaldnsが正常に返していたのに、それを止めてしまったので障害になったのだと思われます。

時間があればVMwareのテスト環境で再現するか確認してみたいと思います。

