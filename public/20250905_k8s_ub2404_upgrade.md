---
title: Kubernetesクラスターの各ノードをUbuntu 24.04にアップグレードした時のメモ
tags:
  - Ubuntu
  - kubernetes
  - kubespray
private: false
updated_at: '2025-09-05T14:44:00+09:00'
id: 1b905e0b010a1459858f
organization_url_name: null
slide: false
ignorePublish: false
---
# はじめに

以前の記事に概要はまとめているのですが、24.04では遭遇しない状況もあったので新規に記事を書きました。

https://qiita.com/YasuhiroABE/items/455150a9cbc86a136889

Rook/Cephの問題のように利用しているバックエンド(ミドルウェア)を最新版にしていないことで遭遇した問題があったため、テスト系のアップデートを最初に行って進めています。

# 環境

* クラスター#1
  * Fujitsu PRIMERGY TX1310 M3 (CPU: Xeon e3-1225 v6, Memory: 64GB)
* クラスター#2
  * Fujitsu PRIMERGY TX1320 M4 (CPU: Xeon E-2234, Memory: 64GB)

## 以降前

* Ubuntu 22.04.5 LTS (amd64)
* Kubespray v2.27.1 (Kubernetes v1.31.9)

## 以降後

* Ubuntu 24.04.3 LTS
* Kubespray v2.27.1 (Kubernetes v1.31.9) ## 変更なし

# 作業手順

まずWorkerノードから作業を進めます。そのためノードの番号でいうと逆順に node4 → node1 の順番で作業を進めます。

## ノードの停止

k8sからノードを外してから作業を進めます。

```bash:control-planeノードで事前にdorainする
$ kubectl cordon node4
$ kubectl drain node4 --force --ignore-daemonsets
## ↑で失敗した場合はメッセージを確認して"--delete-emptydir-data"オプション付きで実行する
$ kubectl drain node4 --force --ignore-daemonsets --delete-emptydir-data
```

## Ubuntu 24.04にアップグレード

この作業は途中でgnu screenが起動するため、tmuxではなく別のターミナルウィンドで実行しています。

作業の前にパッケージなどを更新します。

```bash:node4で実行
$ sudo apt update
$ sudo apt dist-upgrade
```

パッケージを最新にした状態でノードを再起動し、最新のカーネルで起動している状態にします。

```bash:node4で実行
$ sudo shutdown -r now
```

Ubuntu 22.04にアップグレードします。
この中でGNU screenが起動するので、別のターミナルからsshでnode4にログインし、作業を進めます。

```bash:node4で実行
$ sudo do-release-upgrade -d
```

あとは、基本的に'y'キーなどで作業を進め、設定ファイルは現状のまま変更しない'N'を選択しながら見守ります。

最終的に再起動によって、Ubuntu 24.04に更新を完了します。

1台あたりの作業時間は10分以内に完了しています。

## Ubuntu 24.04として起動してからの作業

Kubesprayを利用してUbuntu 24.04にしたノードに改めて``ansible-playbook``を再度適用します。

```bash:ansibleを実行するホストに移動して実行(エラーが発生する例。実施しないこと)
$ . venv/k8s/bin/activate
(k8s) $ grep kube_version inventory/mycluster/group_vars/k8s_cluster/k8s-cluster.yml
kube_version: v1.31.9
(k8s) $ ansible-playbook upgrade-cluster.yml -b -i inventory/mycluster/inventory.ini -e kube_version=v1.31.9 --limit=node4
...
```

22.04の時にはあったmultus関連のエラーなどもなく進むはずです。

この部分の作業で1台にかかった時間はWorkerノードで、4分半〜5分程度でした。

```bash:
node4             : ok=439  changed=26   unreachable=0    failed=0    skipped=770  rescued=0    ignored=1

Friday 05 September 2025  12:31:26 +0900 (0:00:00.070)       0:04:28.646 ******
===============================================================================

node3             : ok=501  changed=36   unreachable=0    failed=0    skipped=814  rescued=0    ignored=1

Friday 05 September 2025  13:09:23 +0900 (0:00:00.060)       0:04:44.134 ******
===============================================================================
```

Control-Planeノードでは、だいたい8分〜8分半程度でした。

```bash:
node2             : ok=696  changed=49   unreachable=0    failed=0    skipped=1303 rescued=0    ignored=1

Friday 05 September 2025  13:49:50 +0900 (0:00:00.064)       0:08:17.721 ******
===============================================================================
node1             : ok=752  changed=59   unreachable=0    failed=0    skipped=1413 rescued=0    ignored=1

Friday 05 September 2025  14:40:39 +0900 (0:00:00.086)       0:08:35.637 ******
===============================================================================
```

## クラスターへの復旧

最後には忘れずに *uncordon* します。

```bash:control-planeでuncordonする。drain状態からもこれだけで復活する
$ sudo kubectl uncordon node4
```

## 次のノードに移る前の確認作業

Rook/Cephのステータスを確認して、``HEALTH_OK``になっていることを確認します。

```bash:
$ function ceph_status ()
{
    name=$(get_toolbox_podname);
    date;
    sudo kubectl -n rook-ceph exec -it "${name}" -- ceph status
}

$ ceph_status
Fri Sep  5 03:34:49 AM UTC 2025
  cluster:
    id:     74a67737-7c7c-4418-9c97-a770e717e15b
    health: HEALTH_OK
...
```

これで次のノードに移り、最後のControle-Planeノードまで続けます。

# Ubuntu 24.04で遭遇した問題

## Rook/Ceph v1.15で遭遇した問題

アップグレードしてみると``ceph status``の状況がかなり悪くなっていて、調べてみるとCSIドライバがrbd.ko.zstを正しくロードできず正常に動作しないことが判りました。

この障害自体は既知の問題で、issuesを調べると1.15の最新版で解決していることが分かります。

新しいLTSがでた場合は、すぐにアップグレードせずに利用しているミドルウェアなどが対応していることを確認してから進めるべきだったという良い教訓になりました。

## Rook/Cephで遭遇した別の問題

ノードの'/'ファイルシステムの使用率が70%を越えたため、``ceph status``がワーニングをだすようになりました。

## PDBの問題

なぜかPod Duration Budget(pdb)が部分的に設定されていてdrainできないノードがありました。

```text:
evicting pod rook-ceph/rook-ceph-osd-4-5bc978fbc6-v6pmd
error when evicting pods/"rook-ceph-osd-4-5bc978fbc6-v6pmd" -n "rook-ceph" (will retry after 5s): Cannot evict pod as it would v
iolate the pod's disruption budget.
```

設定を削除しても良いと思いますが、今回は.specに``minAvailable: 0``を設定して対応しました。

## jp.archive.ubuntu.comのスローダウン

これまで問題にならなかったのですが、あまりにも問題になったため /etc/apt/sources.list で ``ftp.riken.jp/Linux/ubuntu``を指定しました。
