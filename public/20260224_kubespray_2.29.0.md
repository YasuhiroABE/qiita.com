---
title: Kubespray v2.29.1でクラスターをアップグレードした時のメモ
tags:
  - Ansible
  - kubernetes
  - kubespray
private: false
updated_at: ''
id: null
organization_url_name: null
slide: false
ignorePublish: false
---
# はじめに

Kubespray v2.29.1を適用します。

## 環境

* サーバー: TX120 S3p (CPU: Xeon e3-1230 v2, Memory: 32GB)

### 適用対象の現行バージョン

* Kubespray v2.28.1 (Kubernetes v1.32.8)

# 参考資料

https://qiita.com/YasuhiroABE/items/6a16e3e9ac4f027bd559

https://qiita.com/YasuhiroABE/items/3aaf7ceb314f47cd62d9

https://github.com/kubernetes-sigs/kubespray/blob/303dd1cbc16620ff666f111e59add9d3965a449d/docs/operations/upgrades.md

# 事前確認

作業前に次の点を確認します。

## クラスターの再起動

直近で再起動をしていない場合には最新のカーネルに更新(``sudo apt update && sudo apt upgrade``)した上で再起動します。

```bash:control-planeノードで事前にdorainする
$ sudo kubectl cordon node4
$ sudo kubectl drain node4 --force --ignore-daemonsets
## ↑で失敗した場合はメッセージを確認して"--delete-emptydir-data"オプション付きで実行する
$ sudo kubectl drain node4 --force --ignore-daemonsets --delete-emptydir-data
```

対象ノードを再起動します。

```bash:対象ノード(node4)にssh等でログインし、再起動
$ sudo shutdown -r now
```

再起動してからクラスターに復帰させます。

```bash:control-planeノードでuncordonする
$ sudo kubectl uncordon node4
```

:::note
経験上、再起動せずにansible-playbookを進めるとkubernetes/preinstallのプロセスでハングアップすることがありました。
:::

## RELEASE_NOTESの確認

必ず[Release Notes](https://github.com/kubernetes-sigs/kubespray/releases)の各バージョンに記載されている変更内容を確認してください。

v2.29.0のリリースノードでは/etc/hostsがansibleの管理下から外れるといった変更が明記されていますが、アップグレード時には特に対応はしていません。

## Rook/Cephの状態確認

対象のクラスターではPVCを利用するためにRook/Cephを利用しています。

Rook/Cephを利用している場合には、あらかじめクラスターの状態が、**HEALTH_OK** であることを確認します。

```bash:
$ podname=$(kubectl -n rook-ceph get pod -l app=rook-ceph-tools -o jsonpath='{.items[*].metadata.name}')
$ kubectl -n rook-ceph exec -it "${podname}" -- ceph status
```

## 各ノードが参照するAPI Serverのホスト名を確認

過去にaddons.ymlファイルでkube_vip_services_enabled:を有効にしていたが、現在は無効(false)である場合などは作業中に障害となる可能性があります。

各ノードのAPI ServerへのURLを確認します。

```bash:各ノードで/etc/kubernetes/以下のファイルからAPIサーバーのURLを確認
## kubesprayのansible-playbookを実行しているディレクトリで実行
$ . venv/k8s/bin/activate
(k8s) $ ansible all -i inventory/mycluster/inventory.ini -b -m shell -a 'grep server: /etc/kubernetes/*.conf'
```

もしlb-apiserver.kubernetes.localが指定されていた場合には、kube-vipを有効にしていること、各ノードの/etc/hostsに、lb-apiserver.kubernetes.localに対応したエントリがあることを確認し、このIPアドレスをloopback(127.0.0.1)に設定します。

```bash:
(k8s) $ ansible all -i inventory/mycluster/inventory.ini -b -m shell -a 'grep lb-apiserver /etc/hosts'
## "192.168.56.128"は実際のVIPアドレスに変更すること
(k8s) $ ansible all -i inventory/mycluster/inventory.ini -b -m lineinfile -a 'path=/etc/hosts regexp="^192.168.56.128" line="127.0.0.1 lb-apiserver.kubernetes.local"'
## 全てのkubeletを再起動します
(k8s) $ ansible all -i inventory/mycluster/inventory.ini -b -m systemd -a 'name=kubelet.service state=restarted'
```

これによって

もしkube-vipを無効化しているのであれば、全ノードの/etc/kubernetes/以下の設定ファイルから、lb-apiserver.kubernetes.localを参照している部分について、``localhost``などのloopbackアドレスに変更します。

:::note
kubeletがAPI serverと通信できなくなった場合は/etc/kubernetes/*.confファイルの中からVIP(lb-apiserver.kubernetes.local)を参照している箇所を修正して、systemctlからkubeletを再起動します。

詳細は参考資料にある「Kubespray v2.28.1でクラスターをアップグレードした時のメモ」の後半にある「適用後に発生した問題」のセクションを確認してください。
:::

# 作業手順

基本的な遵守事項は次のとおり。

* masterブランチに変更は加えないこと
* masterブランチから作業用ブランチは作成しないこと
* 作業の度にtagsから作業用ブランチを作成すること
* 変更内容は作業用ブランチにcommitすること

始めて実施する場合にはあらかじめGithubからkubesprayディレクトリをpullすること。

```bash:初回のみ
$ git pull https://github.com/kubernetes-sigs/kubespray.git
```

通常はkubesprayディレクトリが存在するものとして次のステップから開始する。

```bash:
$ cd kubespray
$ git status
## コミットしていないファイルがあるとブランチの作成ができないため、untrackedやchangedなファイルがあればcommitしておく
## masterブランチにcommitしないため日付のブランチを作成してcommitする
$ git checkout -b $(date +%Y%m%d.%H%M%S)_tmp
$ git commit -a -m 'prepare to upgrade v2.29.1'

## 【git statusで差分が表示されなければ、ここからスタート】まずkubesprayの最新のリポジトリに更新
$ git checkout master
$ git pull
## 次のバージョンタグを確認する
$ git tag
## v2.28.xの次のv2.29.1からブランチを作成する
$ git checkout refs/tags/v2.29.1 -b t_v2.29.1
## 先頭の tag: v2.29.1 を確認する
$ git log

## 新しいvenv環境を作成し、環境変数を読み込む
$ rm -rf venv
$ /usr/bin/python3 -m venv venv/k8s
$ . venv/k8s/bin/activate
(k8s) $ pip install -r requirements.txt

## inventory/mycluster を作成する。(もし inventory/mycluster があれば削除する)
(k8s) $ rm -rf inventory/mycluster
(k8s) $ cp -rfp inventory/sample inventory/mycluster
## v2.28.0以降ではinventory/mycluster/inventory.iniファイルを利用する
(k8s) $ git checkout t_v2.28.1 inventory/mycluster/inventory.ini
## .gitignore に !inventory/mycluster を追加する
(k8s) $ vi .gitignore
(k8s) $ git add .gitignore inventory/mycluster
(k8s) $ git commit -m 'Added the inventory/mycluster config directory.'
## 一つ前のバージョン(v2.28.1)との差分を確認する
(k8s) $ git diff t_v2.28.1 inventory/mycluster
## 変更が必要なファイルは group_vars/k8s-cluster/{addons.yml,k8s-cluster.yml,k8s-net-calico.yml} のみです
## container_manager: が変更になるなどの大規模な変更がないか、念のため全体を概観してください
## 変更する際には各ファイルの差分を確認しながら作業を進めていきます
(k8s) $ cd inventory/mycluster/group_vars/k8s_cluster
(k8s) $ git diff t_v2.28.1 addons.yml
(k8s) $ vi addons.yml
(k8s) $ git diff t_v2.28.1 k8s-cluster.yml
(k8s) $ vi k8s-cluster.yml
(k8s) $ git diff t_v2.28.1 k8s-net-calico.yml
(k8s) $ vi k8s-net-calico.yml
## 変更が終ったら差分をコミットし、トップディレクトリに移動します
(k8s) $ git add .
(k8s) $ git commit -m 'Modified addons.yml, k8s-cluster.yml, and k8s-net-calico.yml files.'
(k8s) $ cd ../../../../
## 変更方法をupgrades.mdから確認する。そのままでは古いコマンドなので適宜変更すること
(k8s) $ less docs/operations/upgrades.md
## ansible.cfgを見直し、remote_userなどを環境に合わせて、適宜変更する
(k8s) $ git diff t_v2.28.1 ansible.cfg
## 変更したinventory.iniが前の版から変化していないことを確認する (v2.28.0以降は一つ前と比較する)
(k8s) $ git diff t_v2.28.1 inventory/mycluster/inventory.ini
## 変更したansible.cfgが正常に動くか動作を確認し、同時に"kube_node"などに対応するノードの名前と数が正しいか確認する
(k8s) $ ansible kube_control_plane -i inventory/mycluster/inventory.ini -m command -a 'uname -n'
(k8s) $ ansible etcd -i inventory/mycluster/inventory.ini -m command -a 'uname -n'
(k8s) $ ansible kube_node -i inventory/mycluster/inventory.ini -m command -a 'uname -n'
## 正常に動作したら残りのファイルをコミットする (変更点があればinventory.iniも)
(k8s) $ git add ansible.cfg inventory/mycluster/inventory.ini
(k8s) $ git commit -m 'Updated the configuration for Ansible.'
## kube_version を確認するが、v2.28.1から設定ファイルにデフォルト値が記載されなくなったので、RELEASE_NOTESか、checksums.ymlファイルからサポートされているバージョンを確認する
(k8s) $ grep -C 40 kubelet_checksums roles/kubespray_defaults/vars/main/checksums.yml
## 確認したkube_versionを指定して、upgrade-cluster.ymlを実行する
(k8s) $ ansible-playbook upgrade-cluster.yml -b -i inventory/mycluster/inventory.ini -e kube_version=1.33.7
```

# 既知の問題への対応方法

```bash:upgrade-cluster.ymlの実行に失敗した状態
$ kubectl get node
NAME    STATUS                     ROLES           AGE      VERSION
node1   Ready                      control-plane   4y218d   v1.32.5
node2   Ready                      control-plane   4y218d   v1.32.5
node3   Ready                      <none>          4y218d   v1.31.4
node4   Ready,SchedulingDisabled   <none>          4y128d   v1.31.4
node5   Ready                      <none>          4y128d   v1.31.4
```

エラーになっている理由は、cordonした後に想定している時間内にSchedulingDisabledとならなかった点にありました。
積極的にcordon/uncordonを利用して1台づつ処理する場合は次のような流れになります。

```bash:手動で1台づつupgrade-cluster.ymlを実行する例
## kubectlコマンドが実行できるホストに移動
$ kubectl cordon node4

## ansibleを実行するホストに移動し、--limitを利用してcordonしたノードのみにansible roleを適用する
$ ansible-playbook upgrade-cluster.yml -b -i inventory/mycluster/inventory.ini -e kube_version=v1.39.5 --limit node4

## 再びkubectlコマンドを実行するホストに移動
$ kubectl uncordon node2
```

この他のアップグレードが完了していないノードについても、corodnしてから--limitでそのノードを指定してアップグレードし、uncordonしてクラスターに参加、という流れを繰り返します。

この時にRook/Cephを利用していれば、次のノードに移る前に``ceph status``でHEALTH_OKの状態を都度確認してから作業を続けます。

# さいごに

v2.28.xへの更新作業の時にはkube-vipを利用していることで、ノードの停止(cordon)がうまく出来ない場面に遭遇しました。

今回はうまく対応をまとめて事前に回避することができるようになったと思います。

